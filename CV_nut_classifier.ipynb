{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e83c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Nut Variation Classification\n",
      "Number of classes: 3\n",
      "Class names: ['Chopped,Crushed', 'In-Shell', 'Shelled']\n",
      "Creating nut variation datasets...\n",
      "Creating train dataset...\n",
      "  Chopped,Crushed: 800 images\n",
      "  In-Shell: 800 images\n",
      "  Shelled: 800 images\n",
      "Total train images: 2400\n",
      "Creating test dataset...\n",
      "  Chopped,Crushed: 200 images\n",
      "  In-Shell: 200 images\n",
      "  Shelled: 200 images\n",
      "Total test images: 600\n",
      "DataLoaders created: batch_size=64\n",
      "Testing batch loading...\n",
      "✓ Batch test successful! Images: torch.Size([64, 3, 224, 224]), Labels: torch.Size([64])\n",
      "Creating ResNet model for nut variations...\n",
      "Backbone frozen - only training final layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashsh\\Downloads\\CV_Task\\cv_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashsh\\Downloads\\CV_Task\\cv_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11,178,051\n",
      "Trainable parameters: 1,539\n",
      "Starting nut variation training...\n",
      "Starting training for 10 epochs...\n",
      "------------------------------------------------------------\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashsh\\Downloads\\CV_Task\\cv_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ashsh\\AppData\\Local\\Temp\\ipykernel_20676\\323544891.py:160: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\ashsh\\AppData\\Local\\Temp\\ipykernel_20676\\323544891.py:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15/38, Loss: 0.9814\n",
      "  Batch 30/38, Loss: 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashsh\\AppData\\Local\\Temp\\ipykernel_20676\\323544891.py:215: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9900, Train Acc: 0.5071\n",
      "Test Loss: 0.7942, Test Acc: 0.6600\n",
      "LR: 0.001000, Time: 422.3s\n",
      "Best Test Acc: 0.6600\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10\n",
      "  Batch 15/38, Loss: 0.8223\n",
      "  Batch 30/38, Loss: 0.6583\n",
      "Train Loss: 0.7404, Train Acc: 0.6758\n",
      "Test Loss: 0.6724, Test Acc: 0.7200\n",
      "LR: 0.001000, Time: 389.6s\n",
      "Best Test Acc: 0.7200\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10\n",
      "  Batch 15/38, Loss: 0.6593\n",
      "  Batch 30/38, Loss: 0.5974\n",
      "Train Loss: 0.6426, Train Acc: 0.7392\n",
      "Test Loss: 0.6101, Test Acc: 0.7417\n",
      "LR: 0.001000, Time: 386.7s\n",
      "Best Test Acc: 0.7417\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10\n",
      "  Batch 15/38, Loss: 0.5160\n",
      "  Batch 30/38, Loss: 0.6524\n",
      "Train Loss: 0.5902, Train Acc: 0.7521\n",
      "Test Loss: 0.5699, Test Acc: 0.7517\n",
      "LR: 0.001000, Time: 373.0s\n",
      "Best Test Acc: 0.7517\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10\n",
      "  Batch 15/38, Loss: 0.6451\n",
      "  Batch 30/38, Loss: 0.5981\n",
      "Train Loss: 0.5464, Train Acc: 0.7762\n",
      "Test Loss: 0.5601, Test Acc: 0.7583\n",
      "LR: 0.001000, Time: 433.2s\n",
      "Best Test Acc: 0.7583\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10\n",
      "  Batch 15/38, Loss: 0.5360\n",
      "  Batch 30/38, Loss: 0.4748\n",
      "Train Loss: 0.5370, Train Acc: 0.7754\n",
      "Test Loss: 0.5314, Test Acc: 0.7683\n",
      "LR: 0.001000, Time: 563.8s\n",
      "Best Test Acc: 0.7683\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10\n",
      "  Batch 15/38, Loss: 0.4939\n",
      "  Batch 30/38, Loss: 0.5081\n",
      "Train Loss: 0.5003, Train Acc: 0.7954\n",
      "Test Loss: 0.5293, Test Acc: 0.7700\n",
      "LR: 0.001000, Time: 412.6s\n",
      "Best Test Acc: 0.7700\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10\n",
      "  Batch 15/38, Loss: 0.4864\n",
      "  Batch 30/38, Loss: 0.4418\n",
      "Train Loss: 0.4861, Train Acc: 0.8100\n",
      "Test Loss: 0.5016, Test Acc: 0.7833\n",
      "LR: 0.001000, Time: 382.8s\n",
      "Best Test Acc: 0.7833\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10\n",
      "  Batch 15/38, Loss: 0.5021\n",
      "  Batch 30/38, Loss: 0.4990\n",
      "Train Loss: 0.4686, Train Acc: 0.8079\n",
      "Test Loss: 0.4991, Test Acc: 0.7950\n",
      "LR: 0.001000, Time: 386.4s\n",
      "Best Test Acc: 0.7950\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10\n",
      "  Batch 15/38, Loss: 0.5117\n",
      "  Batch 30/38, Loss: 0.4509\n",
      "Train Loss: 0.4509, Train Acc: 0.8100\n",
      "Test Loss: 0.4813, Test Acc: 0.7900\n",
      "LR: 0.001000, Time: 385.8s\n",
      "Best Test Acc: 0.7950\n",
      "------------------------------------------------------------\n",
      "Training completed! Best test accuracy: 0.7950\n",
      "\n",
      "=== TRAINING COMPLETED ===\n",
      "Model saved to: saved_models/nut_variation_classifier.pth\n",
      "Best test accuracy: 0.7950\n",
      "Classes: ['Chopped,Crushed', 'In-Shell', 'Shelled']\n",
      "\n",
      "To load this model later, use:\n",
      "model, class_names, history = load_nut_model('saved_models/nut_variation_classifier.pth', device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "nut_variations = ['Chopped,Crushed', 'In-Shell', 'Shelled']\n",
    "num_classes = len(nut_variations)\n",
    "nut_folder = r\"C:\\Users\\ashsh\\Downloads\\Nut\"\n",
    "\n",
    "print(f\"Nut Variation Classification\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {nut_variations}\")\n",
    "\n",
    "\n",
    "class NutVariationDataset(Dataset):\n",
    "    def __init__(self, nut_folder, variations, transform=None, train=True, train_split=0.8):\n",
    "        self.nut_folder = nut_folder\n",
    "        self.variations = variations\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {variation: idx for idx, variation in enumerate(variations)}\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"Creating {'train' if train else 'test'} dataset...\")\n",
    "        \n",
    "        for variation in variations:\n",
    "            variation_folder = os.path.join(nut_folder, variation)\n",
    "            if os.path.exists(variation_folder):\n",
    "                images_in_variation = []\n",
    "                for img_name in os.listdir(variation_folder):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(variation_folder, img_name)\n",
    "                        images_in_variation.append(img_path)\n",
    "                \n",
    "                train_imgs, test_imgs = train_test_split(images_in_variation, \n",
    "                                                       train_size=train_split, \n",
    "                                                       random_state=42)\n",
    "                \n",
    "                if train:\n",
    "                    selected_images = train_imgs\n",
    "                else:\n",
    "                    selected_images = test_imgs\n",
    "                \n",
    "                self.image_paths.extend(selected_images)\n",
    "                self.labels.extend([self.class_to_idx[variation]] * len(selected_images))\n",
    "                \n",
    "                print(f\"  {variation}: {len(selected_images)} images\")\n",
    "        \n",
    "        print(f\"Total {'train' if train else 'test'} images: {len(self.image_paths)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=25),  # Good rotation for nuts of different orientations\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02),  # Careful with hue for nut colors\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Scale variation for different nut sizes\n",
    "    transforms.RandomPerspective(distortion_scale=0.1, p=0.3),  # Slight perspective changes\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Creating nut variation datasets...\")\n",
    "train_dataset = NutVariationDataset(nut_folder, nut_variations, \n",
    "                                   transform=train_transforms, train=True)\n",
    "test_dataset = NutVariationDataset(nut_folder, nut_variations, \n",
    "                                  transform=test_transforms, train=False)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created: batch_size={batch_size}\")\n",
    "\n",
    "\n",
    "print(\"Testing batch loading...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"✓ Batch test successful! Images: {sample_batch[0].shape}, Labels: {sample_batch[1].shape}\")\n",
    "\n",
    "\n",
    "class NutVariationClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, pretrained=True, freeze_backbone=True):\n",
    "        super(NutVariationClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.resnet.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"Backbone frozen - only training final layer\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "\n",
    "model = NutVariationClassifier(num_classes=num_classes, pretrained=True, freeze_backbone=True)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "def train_nut_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'learning_rates': []}\n",
    "    best_test_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if (batch_idx + 1) % 15 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        test_running_loss = 0.0\n",
    "        test_running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_running_loss += loss.item() * inputs.size(0)\n",
    "                test_running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        test_loss = test_running_loss / len(test_loader.dataset)\n",
    "        test_acc = test_running_corrects.double() / len(test_loader.dataset)\n",
    "        \n",
    "        scheduler.step(test_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc.item())\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        print(f\"LR: {current_lr:.6f}, Time: {epoch_time:.1f}s\")\n",
    "        print(f\"Best Test Acc: {best_test_acc:.4f}\")\n",
    "        \n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping - no improvement for {early_stop_patience} epochs\")\n",
    "            break\n",
    "            \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Training completed! Best test accuracy: {best_test_acc:.4f}\")\n",
    "    return model, history\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "trained_model, training_history = train_nut_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs\n",
    ")\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "model_save_path = \"saved_models/nut_variation_classifier.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'class_names': nut_variations,\n",
    "    'num_classes': num_classes,\n",
    "    'model_architecture': 'resnet18',\n",
    "    'best_test_accuracy': max(training_history['test_acc']),\n",
    "    'training_history': training_history\n",
    "}, model_save_path)\n",
    "\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(f\"Best test accuracy: {max(training_history['test_acc']):.4f}\")\n",
    "print(f\"Classes: {nut_variations}\")\n",
    "\n",
    "\n",
    "def load_nut_model(model_path, device):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = NutVariationClassifier(num_classes=checkpoint['num_classes'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, checkpoint['class_names'], checkpoint['training_history']\n",
    "\n",
    "print(f\"\\nTo load this model later, use:\")\n",
    "print(f\"model, class_names, history = load_nut_model('{model_save_path}', device)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
