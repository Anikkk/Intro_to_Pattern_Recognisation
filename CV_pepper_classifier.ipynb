{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab57e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pepper Variation Classification\n",
      "Number of classes: 3\n",
      "Class names: ['Diced,Sliced', 'Halved,Deseeded', 'Whole']\n",
      "Creating pepper variation datasets...\n",
      "Creating train dataset...\n",
      "  Diced,Sliced: 804 images\n",
      "  Halved,Deseeded: 830 images\n",
      "  Whole: 814 images\n",
      "Total train images: 2448\n",
      "Creating test dataset...\n",
      "  Diced,Sliced: 202 images\n",
      "  Halved,Deseeded: 208 images\n",
      "  Whole: 204 images\n",
      "Total test images: 614\n",
      "DataLoaders created: batch_size=64\n",
      "Testing batch loading...\n",
      "âœ“ Batch test successful! Images: torch.Size([64, 3, 224, 224]), Labels: torch.Size([64])\n",
      "Creating ResNet model for pepper variations...\n",
      "Backbone frozen - only training final layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashsh\\Downloads\\CV_Task\\cv_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ashsh\\Downloads\\CV_Task\\cv_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11,178,051\n",
      "Trainable parameters: 1,539\n",
      "Starting pepper variation training...\n",
      "Starting training for 10 epochs...\n",
      "------------------------------------------------------------\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashsh\\Downloads\\CV_Task\\cv_env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ashsh\\AppData\\Local\\Temp\\ipykernel_44260\\4289082406.py:159: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\ashsh\\AppData\\Local\\Temp\\ipykernel_44260\\4289082406.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15/39, Loss: 0.9304\n",
      "  Batch 30/39, Loss: 0.7093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashsh\\AppData\\Local\\Temp\\ipykernel_44260\\4289082406.py:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8437, Train Acc: 0.6430\n",
      "Test Loss: 0.6263, Test Acc: 0.7638\n",
      "LR: 0.001000, Time: 607.7s\n",
      "Best Test Acc: 0.7638\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10\n",
      "  Batch 15/39, Loss: 0.5312\n",
      "  Batch 30/39, Loss: 0.4546\n",
      "Train Loss: 0.5178, Train Acc: 0.8305\n",
      "Test Loss: 0.5256, Test Acc: 0.7850\n",
      "LR: 0.001000, Time: 558.2s\n",
      "Best Test Acc: 0.7850\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10\n",
      "  Batch 15/39, Loss: 0.4091\n",
      "  Batch 30/39, Loss: 0.3453\n",
      "Train Loss: 0.4260, Train Acc: 0.8627\n",
      "Test Loss: 0.5023, Test Acc: 0.7932\n",
      "LR: 0.001000, Time: 574.9s\n",
      "Best Test Acc: 0.7932\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10\n",
      "  Batch 15/39, Loss: 0.3995\n",
      "  Batch 30/39, Loss: 0.2899\n",
      "Train Loss: 0.3599, Train Acc: 0.8897\n",
      "Test Loss: 0.3997, Test Acc: 0.8502\n",
      "LR: 0.001000, Time: 539.6s\n",
      "Best Test Acc: 0.8502\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10\n",
      "  Batch 15/39, Loss: 0.2991\n",
      "  Batch 30/39, Loss: 0.3651\n",
      "Train Loss: 0.3280, Train Acc: 0.8922\n",
      "Test Loss: 0.3388, Test Acc: 0.8746\n",
      "LR: 0.001000, Time: 562.8s\n",
      "Best Test Acc: 0.8746\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10\n",
      "  Batch 15/39, Loss: 0.3045\n",
      "  Batch 30/39, Loss: 0.3479\n",
      "Train Loss: 0.3059, Train Acc: 0.9032\n",
      "Test Loss: 0.3106, Test Acc: 0.8893\n",
      "LR: 0.001000, Time: 569.1s\n",
      "Best Test Acc: 0.8893\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10\n",
      "  Batch 15/39, Loss: 0.2571\n",
      "  Batch 30/39, Loss: 0.2945\n",
      "Train Loss: 0.2935, Train Acc: 0.9003\n",
      "Test Loss: 0.2786, Test Acc: 0.9088\n",
      "LR: 0.001000, Time: 566.3s\n",
      "Best Test Acc: 0.9088\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10\n",
      "  Batch 15/39, Loss: 0.2578\n",
      "  Batch 30/39, Loss: 0.3182\n",
      "Train Loss: 0.2745, Train Acc: 0.9105\n",
      "Test Loss: 0.3032, Test Acc: 0.8958\n",
      "LR: 0.001000, Time: 583.9s\n",
      "Best Test Acc: 0.9088\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10\n",
      "  Batch 15/39, Loss: 0.2755\n",
      "  Batch 30/39, Loss: 0.3192\n",
      "Train Loss: 0.2800, Train Acc: 0.9011\n",
      "Test Loss: 0.2779, Test Acc: 0.9039\n",
      "LR: 0.001000, Time: 546.1s\n",
      "Best Test Acc: 0.9088\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10\n",
      "  Batch 15/39, Loss: 0.2933\n",
      "  Batch 30/39, Loss: 0.2046\n",
      "Train Loss: 0.2449, Train Acc: 0.9199\n",
      "Test Loss: 0.2400, Test Acc: 0.9283\n",
      "LR: 0.001000, Time: 542.5s\n",
      "Best Test Acc: 0.9283\n",
      "------------------------------------------------------------\n",
      "Training completed! Best test accuracy: 0.9283\n",
      "\n",
      "=== TRAINING COMPLETED ===\n",
      "Model saved to: saved_models/pepper_variation_classifier.pth\n",
      "Best test accuracy: 0.9283\n",
      "Classes: ['Diced,Sliced', 'Halved,Deseeded', 'Whole']\n",
      "\n",
      "To load this model later, use:\n",
      "model, class_names, history = load_pepper_model('saved_models/pepper_variation_classifier.pth', device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "pepper_variations = ['Diced,Sliced', 'Halved,Deseeded', 'Whole']\n",
    "num_classes = len(pepper_variations)\n",
    "pepper_folder = r\"C:\\Users\\ashsh\\Downloads\\Pepper\"\n",
    "\n",
    "print(f\"Pepper Variation Classification\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {pepper_variations}\")\n",
    "\n",
    "\n",
    "class PepperVariationDataset(Dataset):\n",
    "    def __init__(self, pepper_folder, variations, transform=None, train=True, train_split=0.8):\n",
    "        self.pepper_folder = pepper_folder\n",
    "        self.variations = variations\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {variation: idx for idx, variation in enumerate(variations)}\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"Creating {'train' if train else 'test'} dataset...\")\n",
    "        \n",
    "        for variation in variations:\n",
    "            variation_folder = os.path.join(pepper_folder, variation)\n",
    "            if os.path.exists(variation_folder):\n",
    "                images_in_variation = []\n",
    "                for img_name in os.listdir(variation_folder):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        img_path = os.path.join(variation_folder, img_name)\n",
    "                        images_in_variation.append(img_path)\n",
    "                \n",
    "                train_imgs, test_imgs = train_test_split(images_in_variation, \n",
    "                                                       train_size=train_split, \n",
    "                                                       random_state=42)\n",
    "                \n",
    "                if train:\n",
    "                    selected_images = train_imgs\n",
    "                else:\n",
    "                    selected_images = test_imgs\n",
    "                \n",
    "                self.image_paths.extend(selected_images)\n",
    "                self.labels.extend([self.class_to_idx[variation]] * len(selected_images))\n",
    "                \n",
    "                print(f\"  {variation}: {len(selected_images)} images\")\n",
    "        \n",
    "        print(f\"Total {'train' if train else 'test'} images: {len(self.image_paths)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),  \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3, hue=0.05), \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = PepperVariationDataset(pepper_folder, pepper_variations, \n",
    "                                      transform=train_transforms, train=True)\n",
    "test_dataset = PepperVariationDataset(pepper_folder, pepper_variations, \n",
    "                                     transform=test_transforms, train=False)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"âœ“ Batch test successful! Images: {sample_batch[0].shape}, Labels: {sample_batch[1].shape}\")\n",
    "\n",
    "\n",
    "class PepperVariationClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, pretrained=True, freeze_backbone=True):\n",
    "        super(PepperVariationClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.resnet.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "model = PepperVariationClassifier(num_classes=num_classes, pretrained=True, freeze_backbone=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print parameter info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "def train_pepper_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'learning_rates': []}\n",
    "    best_test_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if (batch_idx + 1) % 15 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        test_running_loss = 0.0\n",
    "        test_running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_running_loss += loss.item() * inputs.size(0)\n",
    "                test_running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        test_loss = test_running_loss / len(test_loader.dataset)\n",
    "        test_acc = test_running_corrects.double() / len(test_loader.dataset)\n",
    "        \n",
    "        scheduler.step(test_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc.item())\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc.item())\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        print(f\"LR: {current_lr:.6f}, Time: {epoch_time:.1f}s\")\n",
    "        print(f\"Best Test Acc: {best_test_acc:.4f}\")\n",
    "        \n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping - no improvement for {early_stop_patience} epochs\")\n",
    "            break\n",
    "            \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"Training completed! Best test accuracy: {best_test_acc:.4f}\")\n",
    "    return model, history\n",
    "\n",
    "print(\"Starting pepper variation training...\")\n",
    "num_epochs = 10\n",
    "\n",
    "trained_model, training_history = train_pepper_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs\n",
    ")\n",
    "\n",
    "# Save model\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "model_save_path = \"saved_models/pepper_variation_classifier.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'class_names': pepper_variations,\n",
    "    'num_classes': num_classes,\n",
    "    'model_architecture': 'resnet18',\n",
    "    'best_test_accuracy': max(training_history['test_acc']),\n",
    "    'training_history': training_history\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"\\n=== TRAINING COMPLETED ===\")\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(f\"Best test accuracy: {max(training_history['test_acc']):.4f}\")\n",
    "print(f\"Classes: {pepper_variations}\")\n",
    "\n",
    "\n",
    "def load_pepper_model(model_path, device):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = PepperVariationClassifier(num_classes=checkpoint['num_classes'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model, checkpoint['class_names'], checkpoint['training_history']\n",
    "\n",
    "\n",
    "print(f\"model, class_names, history = load_pepper_model('{model_save_path}', device)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
