{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca3305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashsh\\Downloads\\CV_Task\\cv_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU Memory: 8.59 GB\n",
      "Loading data...\n",
      "Loaded 231637 recipes and 1132367 interactions\n",
      "Preparing data for training...\n",
      "Dataset has 231637 recipes. Sampling 50000 for training...\n",
      "Pre-processing all recipes (this will be cached for future runs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing recipes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [01:51<00:00, 448.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch size: 32\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1562/1562 [06:30<00:00,  4.00it/s, loss=0.0001, lr=4.17e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.0505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1562/1562 [06:31<00:00,  3.99it/s, loss=0.0001, lr=2.78e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Loss: 0.0001\n",
      "Training completed!\n",
      "Generating recipe embeddings...\n",
      "Loading cached processed recipes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [03:05<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 50000 recipes\n",
      "Model saved to recipe_bert_model.pth\n",
      "\n",
      "==================================================\n",
      "TESTING SEARCH ENGINE\n",
      "==================================================\n",
      "\n",
      "Query: ['healthy', 'quick', 'vegetarian']\n",
      "1. smoky baked beans   originally canary baked beans (Score: 0.820)\n",
      "   Tags: weeknight, time-to-make, course, main-ingredient, cuisine...\n",
      "   Time: 140 minutes\n",
      "2. halvah parfait (Score: 0.802)\n",
      "   Tags: course, cuisine, preparation, 5-ingredients-or-less, desserts...\n",
      "   Time: 510 minutes\n",
      "3. thai style coconut eggnog  no eggs (Score: 0.789)\n",
      "   Tags: ...\n",
      "   Time: 4 minutes\n",
      "4. soda bread (Score: 0.769)\n",
      "   Tags: course, breads...\n",
      "   Time: 55 minutes\n",
      "5. pan haggerty (Score: 0.769)\n",
      "   Tags: 60-minutes-or-less, time-to-make, main-ingredient, preparation, potatoes...\n",
      "   Time: 50 minutes\n",
      "\n",
      "Query: ['mexican', 'spicy', 'chicken']\n",
      "1. smoky baked beans   originally canary baked beans (Score: 0.824)\n",
      "   Tags: weeknight, time-to-make, course, main-ingredient, cuisine...\n",
      "   Time: 140 minutes\n",
      "2. light as a feather cake (Score: 0.816)\n",
      "   Tags: time-to-make, course, main-ingredient, preparation, desserts...\n",
      "   Time: 65 minutes\n",
      "3. nepenthe s famous ambrosia burger (Score: 0.800)\n",
      "   Tags: 30-minutes-or-less, time-to-make, course, main-ingredient, cuisine...\n",
      "   Time: 20 minutes\n",
      "4. how to bake brown rice in the oven (Score: 0.788)\n",
      "   Tags: time-to-make, course, preparation, low-protein, healthy...\n",
      "   Time: 65 minutes\n",
      "5. halvah parfait (Score: 0.782)\n",
      "   Tags: course, cuisine, preparation, 5-ingredients-or-less, desserts...\n",
      "   Time: 510 minutes\n",
      "\n",
      "Query: ['dessert', 'chocolate', 'easy']\n",
      "1. swedish nut torte glomminge torte (Score: 0.639)\n",
      "   Tags: 60-minutes-or-less, time-to-make, cuisine, preparation, for-large-groups...\n",
      "   Time: 45 minutes\n",
      "2. strawberry cake w creamy strawberry icing (Score: 0.632)\n",
      "   Tags: time-to-make, course, main-ingredient, preparation, occasion...\n",
      "   Time: 80 minutes\n",
      "3. light as a feather cake (Score: 0.617)\n",
      "   Tags: time-to-make, course, main-ingredient, preparation, desserts...\n",
      "   Time: 65 minutes\n",
      "4. lean homemade sausage (Score: 0.578)\n",
      "   Tags: course, main-ingredient, occasion, breakfast, pork...\n",
      "   Time: 364 minutes\n",
      "5. mango siberian sunrise (Score: 0.564)\n",
      "   Tags: 15-minutes-or-less, time-to-make, course, preparation, low-protein...\n",
      "   Time: 5 minutes\n",
      "\n",
      "Query: ['pasta', 'italian', 'cheese']\n",
      "1. smoky baked beans   originally canary baked beans (Score: 0.790)\n",
      "   Tags: weeknight, time-to-make, course, main-ingredient, cuisine...\n",
      "   Time: 140 minutes\n",
      "2. halvah parfait (Score: 0.776)\n",
      "   Tags: course, cuisine, preparation, 5-ingredients-or-less, desserts...\n",
      "   Time: 510 minutes\n",
      "3. soda bread (Score: 0.765)\n",
      "   Tags: course, breads...\n",
      "   Time: 55 minutes\n",
      "4. light as a feather cake (Score: 0.764)\n",
      "   Tags: time-to-make, course, main-ingredient, preparation, desserts...\n",
      "   Time: 65 minutes\n",
      "5. thai style coconut eggnog  no eggs (Score: 0.750)\n",
      "   Tags: ...\n",
      "   Time: 4 minutes\n",
      "\n",
      "Query: ['breakfast', 'eggs', 'quick']\n",
      "1. mango siberian sunrise (Score: 0.667)\n",
      "   Tags: 15-minutes-or-less, time-to-make, course, preparation, low-protein...\n",
      "   Time: 5 minutes\n",
      "2. pat s no fail boiled eggs (Score: 0.612)\n",
      "   Tags: 60-minutes-or-less, time-to-make, course, main-ingredient, cuisine...\n",
      "   Time: 35 minutes\n",
      "3. swedish nut torte glomminge torte (Score: 0.607)\n",
      "   Tags: 60-minutes-or-less, time-to-make, cuisine, preparation, for-large-groups...\n",
      "   Time: 45 minutes\n",
      "4. disney crystal palace breakfast lasagna in full (Score: 0.586)\n",
      "   Tags: 60-minutes-or-less, time-to-make, course, main-ingredient, cuisine...\n",
      "   Time: 45 minutes\n",
      "5. strawberry cake w creamy strawberry icing (Score: 0.560)\n",
      "   Tags: time-to-make, course, main-ingredient, preparation, occasion...\n",
      "   Time: 80 minutes\n",
      "\n",
      "Search results saved to 'search_results.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, logging\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, recipes_df, tokenizer, max_length=256, cache_dir='cache'):\n",
    "        self.recipes_df = recipes_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        self.processed_data = self._preprocess_all_recipes()\n",
    "        \n",
    "    def _preprocess_all_recipes(self):\n",
    "        cache_file = os.path.join(self.cache_dir, 'processed_recipes.pkl')\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        \n",
    "        processed_data = []\n",
    "        \n",
    "        for idx in tqdm(range(len(self.recipes_df)), desc=\"Processing recipes\"):\n",
    "            recipe = self.recipes_df.iloc[idx]\n",
    "            text = self._create_recipe_text(recipe)\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            processed_data.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'recipe_id': recipe['id'],\n",
    "                'recipe_name': recipe['name']\n",
    "            })\n",
    "        \n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(processed_data, f)\n",
    "        \n",
    "        return processed_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]\n",
    "    \n",
    "    def _create_recipe_text(self, recipe):\n",
    "        tags = recipe['tags']\n",
    "        if isinstance(tags, str):\n",
    "            try:\n",
    "                tags = ast.literal_eval(tags)\n",
    "            except:\n",
    "                tags = []\n",
    "        \n",
    "        ingredients = recipe['ingredients']\n",
    "        if isinstance(ingredients, str):\n",
    "            try:\n",
    "                ingredients = ast.literal_eval(ingredients)\n",
    "            except:\n",
    "                ingredients = []\n",
    "        \n",
    "        text_parts = [\n",
    "            f\"Recipe: {recipe['name']}\",\n",
    "            f\"Tags: {' '.join(tags)}\",\n",
    "            f\"Ingredients: {' '.join(ingredients)}\",\n",
    "            f\"Description: {recipe['description']}\" if pd.notna(recipe['description']) else \"\",\n",
    "            f\"Time: {recipe['minutes']} minutes\" if pd.notna(recipe['minutes']) else \"\"\n",
    "        ]\n",
    "        \n",
    "        return \" \".join([part for part in text_parts if part])\n",
    "\n",
    "class RecipeBERTModel(nn.Module):\n",
    "    \"\"\"BERT-based model for recipe embedding\"\"\"\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, num_labels=256):\n",
    "        super(RecipeBERTModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for i in range(6): \n",
    "            for param in self.bert.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.projection = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        embeddings = self.projection(pooled_output)\n",
    "        return embeddings\n",
    "\n",
    "class RecipeSearchEngine:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = RecipeBERTModel()\n",
    "        self.model.to(device)\n",
    "        \n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "        \n",
    "        self.recipe_embeddings = None\n",
    "        self.recipes_df = None\n",
    "        \n",
    "    def train_model(self, recipes_df, interactions_df, epochs=5, batch_size=64, learning_rate=2e-5, accumulation_steps=4):\n",
    "        \n",
    "        if len(recipes_df) > 50000:\n",
    "            print(f\"Dataset has {len(recipes_df)} recipes. Sampling 50000 for training...\")\n",
    "            recipes_df = recipes_df.sample(n=50000, random_state=42)\n",
    "        \n",
    "        dataset = RecipeDataset(recipes_df, self.tokenizer, max_length=256)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            # Adjust batch size based on available GPU memory\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            if gpu_memory < 8:  # Less than 8GB\n",
    "                batch_size = min(batch_size, 32)\n",
    "            elif gpu_memory < 16:  # Less than 16GB\n",
    "                batch_size = min(batch_size, 64)\n",
    "            print(f\"Using batch size: {batch_size}\")\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0,  \n",
    "            pin_memory=True if torch.cuda.is_available() else False,  \n",
    "            drop_last=True  \n",
    "        )\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        total_steps = len(dataloader) * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)\n",
    "        \n",
    "        from transformers import get_linear_schedule_with_warmup\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        use_amp = torch.cuda.is_available()\n",
    "        if use_amp:\n",
    "            from torch.cuda.amp import GradScaler, autocast\n",
    "            scaler = GradScaler()\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i, batch in enumerate(progress_bar):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        embeddings = self.model(input_ids, attention_mask)\n",
    "                        loss = self._contrastive_loss(embeddings)\n",
    "                        loss = loss / accumulation_steps\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (i + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                else:\n",
    "                    embeddings = self.model(input_ids, attention_mask)\n",
    "                    loss = self._contrastive_loss(embeddings)\n",
    "                    loss = loss / accumulation_steps\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (i + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item() * accumulation_steps\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item() * accumulation_steps:.4f}',\n",
    "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                })\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "    def _contrastive_loss(self, embeddings, temperature=0.07):\n",
    "        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        similarity_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "        \n",
    "        batch_size = embeddings.shape[0]\n",
    "        labels = torch.arange(batch_size).to(device)\n",
    "        \n",
    "        loss = nn.functional.cross_entropy(similarity_matrix, labels)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def generate_embeddings(self, recipes_df):\n",
    "        self.recipes_df = recipes_df\n",
    "        self.model.eval()\n",
    "        \n",
    "        inference_batch_size = 64 if torch.cuda.is_available() else 32\n",
    "        \n",
    "        dataset = RecipeDataset(recipes_df, self.tokenizer, max_length=128)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=inference_batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=0,  # Avoid multiprocessing issues\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        all_embeddings = []\n",
    "        recipe_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    from torch.cuda.amp import autocast\n",
    "                    with autocast():\n",
    "                        embeddings = self.model(input_ids, attention_mask)\n",
    "                else:\n",
    "                    embeddings = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "                recipe_ids.extend(batch['recipe_id'].numpy())\n",
    "                \n",
    "                if torch.cuda.is_available() and len(all_embeddings) % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        self.recipe_embeddings = np.vstack(all_embeddings)\n",
    "        self.recipe_ids = np.array(recipe_ids)\n",
    "        \n",
    "        print(f\"Generated embeddings for {len(self.recipe_embeddings)} recipes\")\n",
    "        \n",
    "    def search(self, query_tags, top_k=10):\n",
    "        if self.recipe_embeddings is None:\n",
    "            raise ValueError(\"Recipe embeddings not generated. Call generate_embeddings first.\")\n",
    "        \n",
    "        query_text = f\"Tags: {' '.join(query_tags)}\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            query_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            query_embedding = self.model(input_ids, attention_mask).cpu().numpy()\n",
    "        \n",
    "        similarities = cosine_similarity(query_embedding, self.recipe_embeddings)[0]\n",
    "        \n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            recipe_id = self.recipe_ids[idx]\n",
    "            recipe = self.recipes_df[self.recipes_df['id'] == recipe_id].iloc[0]\n",
    "            \n",
    "            results.append({\n",
    "                'id': int(recipe_id),\n",
    "                'name': recipe['name'],\n",
    "                'score': float(similarities[idx]),\n",
    "                'tags': ast.literal_eval(recipe['tags']) if isinstance(recipe['tags'], str) else recipe['tags'],\n",
    "                'ingredients': ast.literal_eval(recipe['ingredients']) if isinstance(recipe['ingredients'], str) else recipe['ingredients'],\n",
    "                'minutes': int(recipe['minutes']) if pd.notna(recipe['minutes']) else None,\n",
    "                'description': recipe['description'] if pd.notna(recipe['description']) else \"\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'recipe_embeddings': self.recipe_embeddings,\n",
    "            'recipe_ids': self.recipe_ids\n",
    "        }, path)\n",
    "\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.recipe_embeddings = checkpoint.get('recipe_embeddings')\n",
    "        self.recipe_ids = checkpoint.get('recipe_ids')\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "def main():\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "    recipes_df = pd.read_csv('RAW_recipes.csv')\n",
    "    interactions_df = pd.read_csv('RAW_Interactions.csv')\n",
    "    \n",
    "    print(f\"Loaded {len(recipes_df)} recipes and {len(interactions_df)} interactions\")\n",
    "    \n",
    "    search_engine = RecipeSearchEngine()\n",
    "    \n",
    "    search_engine.train_model(\n",
    "        recipes_df, \n",
    "        interactions_df, \n",
    "        epochs=2,  \n",
    "        batch_size=32,  \n",
    "        learning_rate=5e-5,  \n",
    "        accumulation_steps=2  \n",
    "    )\n",
    "    \n",
    "    search_engine.generate_embeddings(recipes_df)\n",
    "    \n",
    "    search_engine.save_model('recipe_bert_model.pth')\n",
    "    \n",
    "    test_queries = [\n",
    "        ['healthy', 'quick', 'vegetarian'],\n",
    "        ['mexican', 'spicy', 'chicken'],\n",
    "        ['dessert', 'chocolate', 'easy'],\n",
    "        ['pasta', 'italian', 'cheese'],\n",
    "        ['breakfast', 'eggs', 'quick']\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        results = search_engine.search(query, top_k=5)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. {result['name']} (Score: {result['score']:.3f})\")\n",
    "            if len(result['tags']) > 0:\n",
    "                print(f\"   Tags: {', '.join(result['tags'][:5])}...\")\n",
    "            print(f\"   Time: {result['minutes']} minutes\")\n",
    "    \n",
    "    with open('search_results.json', 'w') as f:\n",
    "        all_results = {}\n",
    "        for query in test_queries:\n",
    "            all_results[' '.join(query)] = search_engine.search(query, top_k=10)\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(\"\\nSearch results saved to 'search_results.json'\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc20cd",
   "metadata": {},
   "source": [
    "## Different LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "GPU Memory: 8.59 GB\n",
      "ðŸš€ Starting enhanced recipe model training...\n",
      "Loading and preprocessing data...\n",
      "Loading cached preprocessed data...\n",
      "Loaded 229636 recipes and 1063999 interactions from cache\n",
      "Training model...\n",
      "Preparing data for training...\n",
      "Training on FULL dataset: 229636 recipes\n",
      "Loading cached processed recipes...\n",
      "Using batch size: 64\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3619/3619 [07:27<00:00,  8.09it/s, loss=0.0001, lr=2.78e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3619/3619 [07:21<00:00,  8.20it/s, loss=0.0001, lr=2.22e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3619/3619 [07:27<00:00,  8.09it/s, loss=0.0001, lr=1.67e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Loss: 0.0001\n",
      "Training completed!\n",
      "Generating embeddings...\n",
      "Generating recipe embeddings...\n",
      "Loading cached processed recipes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2413/2413 [02:26<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 231637 recipes\n",
      "Saving enhanced model...\n",
      "Preparing recipe data for backend...\n",
      "âœ… Enhanced model saved to recipe_distilbert_model.pth\n",
      "âœ… Included 229636 recipe records for backend use\n",
      "âœ… Model ready for Streamlit app!\n",
      "\n",
      "============================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "âœ… Model saved with recipe data included\n",
      "âœ… Ready for Streamlit backend!\n",
      "âœ… No CSV files needed for inference\n",
      "âœ… Model file: recipe_distilbert_model.pth\n",
      "âœ… Recipe count: 229636\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, logging\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "def preprocess_csv_data(recipes_path='RAW_recipes.csv', interactions_path='RAW_interactions.csv', cache_dir='cache'):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    recipes_cache = os.path.join(cache_dir, 'preprocessed_recipes.pkl')\n",
    "    interactions_cache = os.path.join(cache_dir, 'preprocessed_interactions.pkl')\n",
    "    \n",
    "    if os.path.exists(recipes_cache) and os.path.exists(interactions_cache):\n",
    "        print(\"Loading cached preprocessed data...\")\n",
    "        with open(recipes_cache, 'rb') as f:\n",
    "            recipes_df = pickle.load(f)\n",
    "        with open(interactions_cache, 'rb') as f:\n",
    "            interactions_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(recipes_df)} recipes and {len(interactions_df)} interactions from cache\")\n",
    "        return recipes_df, interactions_df\n",
    "    \n",
    "    \n",
    "\n",
    "    recipes_df = pd.read_csv(recipes_path)\n",
    "    interactions_df = pd.read_csv(interactions_path)\n",
    "    recipes_df = preprocess_recipes(recipes_df)\n",
    "    interactions_df = preprocess_interactions(interactions_df, recipes_df)\n",
    "    \n",
    "    with open(recipes_cache, 'wb') as f:\n",
    "        pickle.dump(recipes_df, f)\n",
    "    with open(interactions_cache, 'wb') as f:\n",
    "        pickle.dump(interactions_df, f)\n",
    "    \n",
    "    print(f\"Preprocessed data: {len(recipes_df)} recipes, {len(interactions_df)} interactions\")\n",
    "    return recipes_df, interactions_df\n",
    "\n",
    "def preprocess_recipes(recipes_df):\n",
    "    original_count = len(recipes_df)\n",
    "    \n",
    "    recipes_df = recipes_df.dropna(subset=['name', 'id', 'tags', 'ingredients'])\n",
    "    def parse_tags(tags_str):\n",
    "        if pd.isna(tags_str) or tags_str == '':\n",
    "            return []\n",
    "        try:\n",
    "            if isinstance(tags_str, str):\n",
    "                tags = ast.literal_eval(tags_str)\n",
    "                if isinstance(tags, list):\n",
    "                    return [tag.lower().strip() for tag in tags if isinstance(tag, str)]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def parse_ingredients(ingredients_str):\n",
    "        if pd.isna(ingredients_str) or ingredients_str == '':\n",
    "            return []\n",
    "        try:\n",
    "            if isinstance(ingredients_str, str):\n",
    "                ingredients = ast.literal_eval(ingredients_str)\n",
    "                if isinstance(ingredients, list):\n",
    "                    return [ing.lower().strip() for ing in ingredients if isinstance(ing, str)]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    recipes_df['parsed_tags'] = recipes_df['tags'].apply(parse_tags)\n",
    "    recipes_df['parsed_ingredients'] = recipes_df['ingredients'].apply(parse_ingredients)\n",
    "    \n",
    "    recipes_df = recipes_df[\n",
    "        (recipes_df['parsed_tags'].apply(len) > 0) & \n",
    "        (recipes_df['parsed_ingredients'].apply(len) > 0)\n",
    "    ]\n",
    "    \n",
    "    recipes_df['description'] = recipes_df['description'].fillna('')\n",
    "    \n",
    "    recipes_df['minutes'] = pd.to_numeric(recipes_df['minutes'], errors='coerce')\n",
    "    recipes_df['minutes'] = recipes_df['minutes'].fillna(0)\n",
    "    \n",
    "    recipes_df = recipes_df[recipes_df['minutes'] <= 1440] \n",
    "    return recipes_df\n",
    "\n",
    "def preprocess_interactions(interactions_df, recipes_df):\n",
    "    original_count = len(interactions_df)\n",
    "    interactions_df = interactions_df.dropna(subset=['user_id', 'recipe_id', 'rating'])\n",
    "    interactions_df = interactions_df[\n",
    "        (interactions_df['rating'] >= 1) & \n",
    "        (interactions_df['rating'] <= 5)\n",
    "    ]\n",
    "    \n",
    "    valid_recipe_ids = set(recipes_df['id'].values)\n",
    "    interactions_df = interactions_df[interactions_df['recipe_id'].isin(valid_recipe_ids)]\n",
    "    interactions_df['user_id'] = pd.to_numeric(interactions_df['user_id'], errors='coerce')\n",
    "    interactions_df['recipe_id'] = pd.to_numeric(interactions_df['recipe_id'], errors='coerce')\n",
    "    interactions_df['rating'] = pd.to_numeric(interactions_df['rating'], errors='coerce')\n",
    "    interactions_df = interactions_df.dropna(subset=['user_id', 'recipe_id', 'rating'])\n",
    "    \n",
    "    return interactions_df\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, recipes_df, tokenizer, max_length=128, cache_dir='cache'):\n",
    "        self.recipes_df = recipes_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        self.processed_data = self._preprocess_all_recipes()\n",
    "        \n",
    "    def _preprocess_all_recipes(self):\n",
    "        cache_file = os.path.join(self.cache_dir, 'processed_recipes_distilbert.pkl')\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        processed_data = []\n",
    "        \n",
    "        for idx in tqdm(range(len(self.recipes_df)), desc=\"Processing recipes\"):\n",
    "            recipe = self.recipes_df.iloc[idx]\n",
    "            text = self._create_recipe_text(recipe)\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            processed_data.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'recipe_id': recipe['id'],\n",
    "                'recipe_name': recipe['name']\n",
    "            })\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(processed_data, f)\n",
    "        \n",
    "        return processed_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]\n",
    "    \n",
    "    def _create_recipe_text(self, recipe):\n",
    "        tags = recipe['parsed_tags'] if 'parsed_tags' in recipe else []\n",
    "        ingredients = recipe['parsed_ingredients'] if 'parsed_ingredients' in recipe else []\n",
    "        \n",
    "        if not tags:\n",
    "            tags_raw = recipe['tags']\n",
    "            if isinstance(tags_raw, str):\n",
    "                try:\n",
    "                    tags = ast.literal_eval(tags_raw)\n",
    "                    tags = [tag.lower().strip() for tag in tags if isinstance(tag, str)]\n",
    "                except:\n",
    "                    tags = []\n",
    "        \n",
    "        if not ingredients:\n",
    "            ingredients_raw = recipe['ingredients']\n",
    "            if isinstance(ingredients_raw, str):\n",
    "                try:\n",
    "                    ingredients = ast.literal_eval(ingredients_raw)\n",
    "                    ingredients = [ing.lower().strip() for ing in ingredients if isinstance(ing, str)]\n",
    "                except:\n",
    "                    ingredients = []\n",
    "        \n",
    "        text_parts = [\n",
    "            f\"Recipe: {recipe['name']}\",\n",
    "            f\"Tags: {' '.join(tags[:15])}\",  \n",
    "            f\"Ingredients: {' '.join(ingredients[:10])}\",  \n",
    "        ]\n",
    "        \n",
    "        if pd.notna(recipe['description']) and len(str(recipe['description'])) < 150:\n",
    "            text_parts.append(f\"Description: {recipe['description']}\")\n",
    "            \n",
    "        if pd.notna(recipe['minutes']) and recipe['minutes'] > 0:\n",
    "            text_parts.append(f\"Time: {int(recipe['minutes'])} minutes\")\n",
    "        \n",
    "        return \" \".join([part for part in text_parts if part])\n",
    "\n",
    "class RecipeDistilBERTModel(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', hidden_size=768, embedding_dim=256):\n",
    "        super(RecipeDistilBERTModel, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        for i in range(3):  \n",
    "            for param in self.distilbert.transformer.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.projection = nn.Linear(hidden_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        embeddings = self.projection(cls_output)\n",
    "        return embeddings\n",
    "\n",
    "class RecipeSearchEngine:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = RecipeDistilBERTModel()\n",
    "        self.model.to(device)\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.load_model(model_path)\n",
    "        \n",
    "        self.recipe_embeddings = None\n",
    "        self.recipes_df = None\n",
    "        \n",
    "    def train_model(self, recipes_df, interactions_df, epochs=2, batch_size=64, learning_rate=3e-5, accumulation_steps=2):\n",
    "        \n",
    "        self.recipes_df = recipes_df\n",
    "        \n",
    "        dataset = RecipeDataset(recipes_df, self.tokenizer, max_length=128)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            if gpu_memory < 8:\n",
    "                batch_size = min(batch_size, 32)\n",
    "            elif gpu_memory < 16:\n",
    "                batch_size = min(batch_size, 64)\n",
    "            else:\n",
    "                batch_size = min(batch_size, 96)\n",
    "            print(f\"Using batch size: {batch_size}\")\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        total_steps = len(dataloader) * epochs\n",
    "        warmup_steps = int(0.1 * total_steps)\n",
    "        \n",
    "        from transformers import get_linear_schedule_with_warmup\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        use_amp = torch.cuda.is_available()\n",
    "        if use_amp:\n",
    "            from torch.cuda.amp import GradScaler, autocast\n",
    "            scaler = GradScaler()\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i, batch in enumerate(progress_bar):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        embeddings = self.model(input_ids, attention_mask)\n",
    "                        loss = self._contrastive_loss(embeddings)\n",
    "                        loss = loss / accumulation_steps\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (i + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                else:\n",
    "                    embeddings = self.model(input_ids, attention_mask)\n",
    "                    loss = self._contrastive_loss(embeddings)\n",
    "                    loss = loss / accumulation_steps\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (i + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item() * accumulation_steps\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item() * accumulation_steps:.4f}',\n",
    "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                })\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        \n",
    "    def _contrastive_loss(self, embeddings, temperature=0.07):\n",
    "        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "        batch_size = embeddings.shape[0]\n",
    "        labels = torch.arange(batch_size).to(device)\n",
    "        loss = nn.functional.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "    \n",
    "    def generate_embeddings(self, recipes_df):\n",
    "        self.recipes_df = recipes_df\n",
    "        self.model.eval()\n",
    "        \n",
    "        inference_batch_size = 96 if torch.cuda.is_available() else 32\n",
    "        \n",
    "        dataset = RecipeDataset(recipes_df, self.tokenizer, max_length=128)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=inference_batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False,\n",
    "            drop_last=False\n",
    "        )\n",
    "        \n",
    "        all_embeddings = []\n",
    "        recipe_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    from torch.cuda.amp import autocast\n",
    "                    with autocast():\n",
    "                        embeddings = self.model(input_ids, attention_mask)\n",
    "                else:\n",
    "                    embeddings = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "                recipe_ids.extend(batch['recipe_id'].numpy())\n",
    "                \n",
    "                if torch.cuda.is_available() and len(all_embeddings) % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        self.recipe_embeddings = np.vstack(all_embeddings)\n",
    "        self.recipe_ids = np.array(recipe_ids)\n",
    "        \n",
    "        print(f\"Generated embeddings for {len(self.recipe_embeddings)} recipes\")\n",
    "        \n",
    "    def save_model(self, path):\n",
    "\n",
    "        recipes_data = []\n",
    "        if self.recipes_df is not None:\n",
    "            for _, recipe in self.recipes_df.iterrows():\n",
    "                recipe_dict = {\n",
    "                    'id': int(recipe['id']),\n",
    "                    'name': str(recipe['name']),\n",
    "                    'minutes': int(recipe.get('minutes', 0)) if pd.notna(recipe.get('minutes')) else 0,\n",
    "                    'description': str(recipe.get('description', '')),\n",
    "                    'n_steps': int(recipe.get('n_steps', 0)) if pd.notna(recipe.get('n_steps')) else 0,\n",
    "                    'parsed_tags': recipe.get('parsed_tags', []),\n",
    "                    'parsed_ingredients': recipe.get('parsed_ingredients', []),\n",
    "                    # Keep original formats as backup\n",
    "                    'tags': recipe.get('tags', []),\n",
    "                    'ingredients': recipe.get('ingredients', [])\n",
    "                }\n",
    "                recipes_data.append(recipe_dict)\n",
    "        \n",
    "        save_data = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'recipe_embeddings': self.recipe_embeddings,\n",
    "            'recipe_ids': self.recipe_ids,\n",
    "            'recipes_data': recipes_data  # THIS IS THE KEY ADDITION!\n",
    "        }\n",
    "        \n",
    "        torch.save(save_data, path)\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.recipe_embeddings = checkpoint.get('recipe_embeddings')\n",
    "        self.recipe_ids = checkpoint.get('recipe_ids')\n",
    "\n",
    "def main():\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "    \n",
    "    print(\"Starting enhanced recipe model training...\")\n",
    "    \n",
    "    recipes_df, interactions_df = preprocess_csv_data()\n",
    "    search_engine = RecipeSearchEngine()\n",
    "    \n",
    "    search_engine.train_model(\n",
    "        recipes_df, \n",
    "        interactions_df,\n",
    "        epochs=3,\n",
    "        batch_size=64,\n",
    "        learning_rate=3e-5,\n",
    "        accumulation_steps=2\n",
    "    )\n",
    "    search_engine.generate_embeddings(recipes_df)\n",
    "    search_engine.save_model('recipe_distilbert_model.pth')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
